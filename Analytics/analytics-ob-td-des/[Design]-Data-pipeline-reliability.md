
# Introduction
This document outlines the design to manage metrics generated by various components in the data pipeline and make way for other systems to take action on it. The metrics are important to make sure the system is stable and reliable. The reliability of the data pipeline can be achieved based on the timely metrics from different components and how quickly we are acting on the alerts/warning to resolve the issues to minimize the damage to the system.




# Problem Statement
The reliability of data is crucial for business and other stakeholders. The accuracy of reports and other data metrics relies on the consistency of pipeline jobs. For example, The reports based on device location is dependent on the "location updater" samza job where it stamps the event with the device location. If this job fails to stamp the location details for some of the events, there would be inconsistency in the reports and fixing those reports would require a lot of effort. Detecting anomaly such as this would require constant monitoring of the pipeline job and alert on abnormal behavior.  We need to have a system to solve the data consistency issues in real-time.


# High-level design
![](images/storage/Untitled%20Diagram%20(5).png)





The high-level diagram shows the different systems generating and pushing metrics to the common Kafka topic. Metrics from Kafka can be pushed to Prometheus to set rules and to manage alerts. We can set up dashboards to visualize on Grafana. Alerting mechanism happens in streaming fashion, meaning as and when we get the metric into the monitoring system.

 **Job manager & Data Products:**  

Data products are scheduled batch jobs run every day. Before running these batch jobs we have to make sure that the system is in normal state meaning data is synced and there are no issues with data correctness. In order to check that, the Job-manager can run "Data Audit job" which can check the data consistency using pre-defined checks (using Prometheus). Only after these checks are passed, Job-Manager can continue with other data products.  

The job manager sends the metric after each job runs to the Kafka topic. It also sends job logs for each job status when it starts/stops to indicate the status of the job so that the dependent job can run when a parent job is done. 


# Metric Event Structure
Here we are introducing a new event ID to the V3 telemetry specification, i.e METRIC event and it has below event structure. All components should send metric with this below event structure

 **Metric**  event consists of following fields in the  _edata_  section:


1. System: High level system ID from which metrics are being generated. This field is mandatory.
1. Subsystem: self-contained system ID within a High level system. This field is optional.
1. Metrics: consists of metric values and its type. This field is mandatory.
1. frequency: to denote the frequency of this metric being generated. This is an optional field. 



 **Event ID(EID): METRIC** 


```
// METRIC event
{
    "edata": {
        "system": "", // High level system e.g "Secor", "Data Pipeline", "Data Products", "Analytics API" . // required
        "subsystem": "", // sub system type e.g "Denormalization", "WFS", "Unique BackUp"       // Optional
        "metrics": [{ // metrics  // required
            "metric": "", // metric e.g "count", "inputCount", "outputCount", "consumerLag", "timeTaken"
            "value": , // value for the metric
            "range": {"min": , "max": "" } // Optional. Rule if any e.g Range("1000000", "1500000")
        }],
        "frequency": // Frequency of metric event generation in mins e.g 5 - pipeline metrics, 1440 - data products // Optional
    }
}

```



### Job Status logs:
Job Status can be tracked using LOG events using the below LOG event spec.  


```
// LOG event
{
  "edata": {
    "type": "", // Required. Type of log (job-manager, data-products, etc)
    "level": "", // Required. Level of the log. TRACE, DEBUG, INFO, WARN, ERROR, FATAL
    "message": "", // Required. Log message
    "pageid": "", // Optional. Page where the log event has happened
    "params": [{"key":"value"}] // Optional. Additional params in the log message
  }
}  
```



# Job execution based on health check metrics:


![](images/storage/Untitled%20Diagram%20(4).png)

As of now, Data products(Job) are run as scheduled without checking the health of data. This causes incorrect results in data product output when the health of data is poor. To avoid this, we can check the health of data using pre-defined rules in the audit job. The Audit job also helps to validate the overall health of data in the system and it can be used by Job-manager to stop executing the other jobs if the health check fails.  The health check report from Audit job can be used to check the rules against the individual data product prerequisite (rules) before executing the job.


# Data Monitoring:
List of data monitoring items identified in our system:


1.  **Pipeline failed events monitoring** 
1.  **Pipeline Lag monitoring** 
1.  **Secor backup monitoring** 
1.  **Data product monitoring** 
1.  **Data in Druid vs Cloud** 


## 1. Pipeline failed events monitoring: 
Ideally, there should be no failed events in the pipeline however, in reality, there will be failed events due to many reasons. Events are pushed to the failed bucket when there is a validation failure/parsing issue/event size greater than the allowed size.  Currently, we have no mechanism to fix the failed events and republish back to the main pipeline. Events failing every day should be monitored and alert the particular component owner to fix the issue at the source otherwise we would miss crucial amount of data. 


## 2. Pipeline Lag Monitoring:
Pipeline streaming jobs process events as and when it comes into the pipeline. Pipeline lag happens when there is a sudden increase in data. The no. of containers running for each job has fixed throughput beyond which it cannot process events faster to clear the lag. In order to detect this situation, we have to send lag metrics for each job and monitor if it exceeds a certain limit.


## 3. Secor backup monitoring:
Secor is an important component in the pipeline. It helps to keep the data into a persistent store (Azure blob storage). Many data products directly read from Azure blob storage to process the data for summarizing/reporting purposes. The Job manager should not execute the data product if the data is not backed up by the Secor otherwise it would result in incorrect data reporting from data products. Monitoring the Secor backup process is important to make sure the data is backed completely.


## 4. Data product monitoring:
Data product should run as scheduled every day without fail. The Data product life cycle events such as started/running/stopped are logged for monitoring purposes. At the end of data product run it should generate a metric event to summarize its output results. This helps to detect abnormal behavior in data product due to bug. The execution status can help to resolve the dependency between jobs. The Job status can help when a dependent job is waiting to be executed only after parent job ends. This only works when there is no asynchronous task involved between (such as uploading files to cloud, indexing it to Elasticsearch) the time parent job finishes its work and the dependent job starts. 


## 5. Data in Druid vs Cloud: 
Data is ingested to Druid after the data is processed in the pipeline. The backups are uploaded to cloud after the each samza job output the data. The count between the data in Druid and backup should almost match, there might be a difference in the count due to validation errors in Druid where some events are not ingested. This difference should be within the permissible limits. We should monitor this difference periodically and alert when it exceeds certain limit. 



















*****

[[category.storage-team]] 
[[category.confluence]] 
